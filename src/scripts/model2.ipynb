{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-17 13:49:53.751077: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-17 13:49:53.929977: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-17 13:49:53.930020: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-17 13:49:55.182699: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-17 13:49:55.182801: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-17 13:49:55.182811: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17541, 100, 3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.load(f'data/rectangle_1/04_training_data/X_norm.npy', allow_pickle=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 100, 3)]     0           []                               \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 50, 32)       320         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 25, 64)       6208        ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 1600)         0           ['conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 16)           25616       ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " z_mean (Dense)                 (None, 64)           1088        ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " z_log_var (Dense)              (None, 64)           1088        ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " sampling (Sampling)            (None, 64)           0           ['z_mean[0][0]',                 \n",
      "                                                                  'z_log_var[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 34,320\n",
      "Trainable params: 34,320\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-17 13:50:00.092327: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-03-17 13:50:00.092390: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-03-17 13:50:00.092427: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (srv-lab-t-251): /proc/driver/nvidia/version does not exist\n",
      "2023-03-17 13:50:00.092697: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 64\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(100, 3))\n",
    "x = layers.Conv1D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\n",
    "x = layers.Conv1D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(16, activation=\"relu\")(x)\n",
    "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "z = Sampling()([z_mean, z_log_var])\n",
    "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 64)]              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1600)              104000    \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 25, 64)            0         \n",
      "                                                                 \n",
      " conv1d_transpose (Conv1DTra  (None, 50, 64)           12352     \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " conv1d_transpose_1 (Conv1DT  (None, 100, 32)          6176      \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv1d_transpose_2 (Conv1DT  (None, 100, 3)           291       \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 122,819\n",
      "Trainable params: 122,819\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "x = layers.Dense(25*64, activation=\"relu\")(latent_inputs) # 25 = 100/4 (due to strides=2 in encoder)\n",
    "x = layers.Reshape((25, 64))(x)\n",
    "x = layers.Conv1DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "x = layers.Conv1DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "decoder_outputs = layers.Conv1DTranspose(3, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
    "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                tf.reduce_sum(\n",
    "                    keras.losses.binary_crossentropy(data, reconstruction), axis=-1\n",
    "                )\n",
    "            )\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "138/138 [==============================] - 4s 14ms/step - loss: 61.1393 - reconstruction_loss: 61.2808 - kl_loss: 1.1210e-04\n",
      "Epoch 2/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.2742 - reconstruction_loss: 61.2754 - kl_loss: 1.0098e-04\n",
      "Epoch 3/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.3990 - reconstruction_loss: 61.3344 - kl_loss: 1.2986e-04\n",
      "Epoch 4/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.3095 - reconstruction_loss: 61.3745 - kl_loss: 9.9320e-05\n",
      "Epoch 5/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.3485 - reconstruction_loss: 61.3499 - kl_loss: 1.0139e-04\n",
      "Epoch 6/500\n",
      "138/138 [==============================] - 2s 15ms/step - loss: 61.2583 - reconstruction_loss: 61.2746 - kl_loss: 1.1547e-04\n",
      "Epoch 7/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.2453 - reconstruction_loss: 61.3062 - kl_loss: 1.2647e-04\n",
      "Epoch 8/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.3289 - reconstruction_loss: 61.2546 - kl_loss: 6.2614e-05\n",
      "Epoch 9/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.3613 - reconstruction_loss: 61.3667 - kl_loss: 9.6032e-05\n",
      "Epoch 10/500\n",
      "138/138 [==============================] - 2s 15ms/step - loss: 61.1108 - reconstruction_loss: 61.3103 - kl_loss: 1.0342e-04\n",
      "Epoch 11/500\n",
      "138/138 [==============================] - 2s 15ms/step - loss: 61.4920 - reconstruction_loss: 61.3221 - kl_loss: 1.3651e-04\n",
      "Epoch 12/500\n",
      "138/138 [==============================] - 2s 16ms/step - loss: 61.3573 - reconstruction_loss: 61.3075 - kl_loss: 6.1182e-05\n",
      "Epoch 13/500\n",
      "138/138 [==============================] - 2s 15ms/step - loss: 61.4720 - reconstruction_loss: 61.3048 - kl_loss: 7.0046e-05\n",
      "Epoch 14/500\n",
      "138/138 [==============================] - 2s 15ms/step - loss: 61.2294 - reconstruction_loss: 61.2734 - kl_loss: 1.5260e-04\n",
      "Epoch 15/500\n",
      "138/138 [==============================] - 2s 15ms/step - loss: 61.3346 - reconstruction_loss: 61.3194 - kl_loss: 1.0314e-04\n",
      "Epoch 16/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.2956 - reconstruction_loss: 61.2867 - kl_loss: 1.0501e-04\n",
      "Epoch 17/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.2529 - reconstruction_loss: 61.3262 - kl_loss: 1.0759e-04\n",
      "Epoch 18/500\n",
      "138/138 [==============================] - 2s 11ms/step - loss: 61.2850 - reconstruction_loss: 61.3588 - kl_loss: 1.5016e-04\n",
      "Epoch 19/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.2526 - reconstruction_loss: 61.3062 - kl_loss: 1.3466e-04\n",
      "Epoch 20/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.3244 - reconstruction_loss: 61.2461 - kl_loss: 7.6150e-05\n",
      "Epoch 21/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.2375 - reconstruction_loss: 61.3630 - kl_loss: 5.9992e-05\n",
      "Epoch 22/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.2858 - reconstruction_loss: 61.2388 - kl_loss: 5.1494e-05\n",
      "Epoch 23/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.2240 - reconstruction_loss: 61.3013 - kl_loss: 1.2320e-04\n",
      "Epoch 24/500\n",
      "138/138 [==============================] - 2s 15ms/step - loss: 61.2812 - reconstruction_loss: 61.2685 - kl_loss: 1.0025e-04\n",
      "Epoch 25/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.2887 - reconstruction_loss: 61.3409 - kl_loss: 8.4250e-05\n",
      "Epoch 26/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.1183 - reconstruction_loss: 61.3391 - kl_loss: 8.6025e-05\n",
      "Epoch 27/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.3992 - reconstruction_loss: 61.2844 - kl_loss: 8.0726e-05\n",
      "Epoch 28/500\n",
      "138/138 [==============================] - 2s 15ms/step - loss: 61.0288 - reconstruction_loss: 61.2818 - kl_loss: 6.3274e-05\n",
      "Epoch 29/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.2603 - reconstruction_loss: 61.3068 - kl_loss: 1.2163e-04\n",
      "Epoch 30/500\n",
      "138/138 [==============================] - 2s 15ms/step - loss: 61.1007 - reconstruction_loss: 61.2544 - kl_loss: 8.9621e-05\n",
      "Epoch 31/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.4435 - reconstruction_loss: 61.3445 - kl_loss: 6.0716e-05\n",
      "Epoch 32/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.0611 - reconstruction_loss: 61.3621 - kl_loss: 6.2833e-05\n",
      "Epoch 33/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.0122 - reconstruction_loss: 61.2920 - kl_loss: 1.0860e-04\n",
      "Epoch 34/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.4696 - reconstruction_loss: 61.2711 - kl_loss: 1.0557e-04\n",
      "Epoch 35/500\n",
      "138/138 [==============================] - 2s 11ms/step - loss: 61.2446 - reconstruction_loss: 61.3273 - kl_loss: 1.1695e-04\n",
      "Epoch 36/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.1462 - reconstruction_loss: 61.3013 - kl_loss: 1.3061e-04\n",
      "Epoch 37/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.2235 - reconstruction_loss: 61.2612 - kl_loss: 7.6454e-05\n",
      "Epoch 38/500\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 61.3694 - reconstruction_loss: 61.2945 - kl_loss: 6.4291e-05\n",
      "Epoch 39/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.2979 - reconstruction_loss: 61.3882 - kl_loss: 5.3226e-05\n",
      "Epoch 40/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.4448 - reconstruction_loss: 61.2798 - kl_loss: 8.3391e-05\n",
      "Epoch 41/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.3255 - reconstruction_loss: 61.3260 - kl_loss: 6.3712e-05\n",
      "Epoch 42/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.1539 - reconstruction_loss: 61.3381 - kl_loss: 8.8580e-05\n",
      "Epoch 43/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.2677 - reconstruction_loss: 61.2561 - kl_loss: 6.4076e-05\n",
      "Epoch 44/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.2425 - reconstruction_loss: 61.3365 - kl_loss: 3.8971e-05\n",
      "Epoch 45/500\n",
      "138/138 [==============================] - 2s 15ms/step - loss: 61.3885 - reconstruction_loss: 61.2605 - kl_loss: 6.7631e-05\n",
      "Epoch 46/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.2840 - reconstruction_loss: 61.3301 - kl_loss: 9.7311e-05\n",
      "Epoch 47/500\n",
      "138/138 [==============================] - 2s 15ms/step - loss: 61.3078 - reconstruction_loss: 61.2939 - kl_loss: 1.0187e-04\n",
      "Epoch 48/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.4081 - reconstruction_loss: 61.2728 - kl_loss: 6.2863e-05\n",
      "Epoch 49/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.3654 - reconstruction_loss: 61.2698 - kl_loss: 5.5407e-05\n",
      "Epoch 50/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.2552 - reconstruction_loss: 61.2606 - kl_loss: 5.7150e-05\n",
      "Epoch 51/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.4034 - reconstruction_loss: 61.2775 - kl_loss: 6.3462e-05\n",
      "Epoch 52/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.3816 - reconstruction_loss: 61.3218 - kl_loss: 4.6367e-05\n",
      "Epoch 53/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.4089 - reconstruction_loss: 61.2898 - kl_loss: 5.0600e-05\n",
      "Epoch 54/500\n",
      "138/138 [==============================] - 2s 15ms/step - loss: 61.4480 - reconstruction_loss: 61.3332 - kl_loss: 6.8691e-05\n",
      "Epoch 55/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.4564 - reconstruction_loss: 61.2698 - kl_loss: 9.8652e-05\n",
      "Epoch 56/500\n",
      "138/138 [==============================] - 2s 15ms/step - loss: 61.3295 - reconstruction_loss: 61.2703 - kl_loss: 7.8395e-05\n",
      "Epoch 57/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.2672 - reconstruction_loss: 61.2137 - kl_loss: 5.6623e-05\n",
      "Epoch 58/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.4330 - reconstruction_loss: 61.3308 - kl_loss: 4.1926e-05\n",
      "Epoch 59/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.2441 - reconstruction_loss: 61.3034 - kl_loss: 8.1608e-05\n",
      "Epoch 60/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.3050 - reconstruction_loss: 61.2871 - kl_loss: 4.9862e-05\n",
      "Epoch 61/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.1971 - reconstruction_loss: 61.2249 - kl_loss: 6.5993e-05\n",
      "Epoch 62/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.4603 - reconstruction_loss: 61.2732 - kl_loss: 4.1979e-05\n",
      "Epoch 63/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.3184 - reconstruction_loss: 61.2743 - kl_loss: 3.8787e-05\n",
      "Epoch 64/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.3811 - reconstruction_loss: 61.2657 - kl_loss: 7.5399e-05\n",
      "Epoch 65/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.3669 - reconstruction_loss: 61.3014 - kl_loss: 7.5811e-05\n",
      "Epoch 66/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.3271 - reconstruction_loss: 61.2519 - kl_loss: 5.7577e-05\n",
      "Epoch 67/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.3781 - reconstruction_loss: 61.2355 - kl_loss: 5.3698e-05\n",
      "Epoch 68/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.3794 - reconstruction_loss: 61.3129 - kl_loss: 4.8051e-05\n",
      "Epoch 69/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.3063 - reconstruction_loss: 61.3528 - kl_loss: 7.5892e-05\n",
      "Epoch 70/500\n",
      "138/138 [==============================] - 2s 15ms/step - loss: 61.4266 - reconstruction_loss: 61.2614 - kl_loss: 9.7007e-05\n",
      "Epoch 71/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.3821 - reconstruction_loss: 61.2982 - kl_loss: 7.7576e-05\n",
      "Epoch 72/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.2967 - reconstruction_loss: 61.3280 - kl_loss: 7.2976e-05\n",
      "Epoch 73/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.3862 - reconstruction_loss: 61.3139 - kl_loss: 6.3908e-05\n",
      "Epoch 74/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.4871 - reconstruction_loss: 61.2460 - kl_loss: 6.5072e-05\n",
      "Epoch 75/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.2227 - reconstruction_loss: 61.2557 - kl_loss: 3.0959e-05\n",
      "Epoch 76/500\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 61.2957 - reconstruction_loss: 61.3899 - kl_loss: 7.0813e-05\n",
      "Epoch 77/500\n",
      "138/138 [==============================] - 2s 11ms/step - loss: 61.3350 - reconstruction_loss: 61.3940 - kl_loss: 6.2755e-05\n",
      "Epoch 78/500\n",
      "138/138 [==============================] - 2s 11ms/step - loss: 61.2299 - reconstruction_loss: 61.2184 - kl_loss: 4.7247e-05\n",
      "Epoch 79/500\n",
      "138/138 [==============================] - 2s 11ms/step - loss: 61.1977 - reconstruction_loss: 61.2449 - kl_loss: 3.3111e-05\n",
      "Epoch 80/500\n",
      "138/138 [==============================] - 1s 11ms/step - loss: 61.3721 - reconstruction_loss: 61.2443 - kl_loss: 3.3876e-05\n",
      "Epoch 81/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.1789 - reconstruction_loss: 61.2482 - kl_loss: 3.7920e-05\n",
      "Epoch 82/500\n",
      "138/138 [==============================] - 1s 11ms/step - loss: 61.4558 - reconstruction_loss: 61.2597 - kl_loss: 2.5789e-05\n",
      "Epoch 83/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.4331 - reconstruction_loss: 61.2632 - kl_loss: 3.4574e-05\n",
      "Epoch 84/500\n",
      "138/138 [==============================] - 2s 15ms/step - loss: 61.2542 - reconstruction_loss: 61.3130 - kl_loss: 4.4135e-05\n",
      "Epoch 85/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.1998 - reconstruction_loss: 61.2153 - kl_loss: 3.6821e-05\n",
      "Epoch 86/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.2496 - reconstruction_loss: 61.3490 - kl_loss: 4.0174e-05\n",
      "Epoch 87/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.3362 - reconstruction_loss: 61.2262 - kl_loss: 4.4724e-05\n",
      "Epoch 88/500\n",
      "138/138 [==============================] - 2s 11ms/step - loss: 61.2545 - reconstruction_loss: 61.2496 - kl_loss: 4.1094e-05\n",
      "Epoch 89/500\n",
      "138/138 [==============================] - 2s 15ms/step - loss: 61.2763 - reconstruction_loss: 61.4553 - kl_loss: 3.9564e-05\n",
      "Epoch 90/500\n",
      "138/138 [==============================] - 2s 15ms/step - loss: 61.1787 - reconstruction_loss: 61.3848 - kl_loss: 4.4196e-05\n",
      "Epoch 91/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.3678 - reconstruction_loss: 61.2632 - kl_loss: 4.3940e-05\n",
      "Epoch 92/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.3534 - reconstruction_loss: 61.2654 - kl_loss: 5.7415e-05\n",
      "Epoch 93/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.2794 - reconstruction_loss: 61.2646 - kl_loss: 2.8079e-05\n",
      "Epoch 94/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.1630 - reconstruction_loss: 61.3684 - kl_loss: 3.9539e-05\n",
      "Epoch 95/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.4092 - reconstruction_loss: 61.3016 - kl_loss: 5.7215e-05\n",
      "Epoch 96/500\n",
      "138/138 [==============================] - 2s 11ms/step - loss: 61.3865 - reconstruction_loss: 61.3093 - kl_loss: 3.7795e-05\n",
      "Epoch 97/500\n",
      "138/138 [==============================] - 1s 11ms/step - loss: 61.4453 - reconstruction_loss: 61.3310 - kl_loss: 4.1249e-05\n",
      "Epoch 98/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.1160 - reconstruction_loss: 61.2082 - kl_loss: 5.0526e-05\n",
      "Epoch 99/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.2883 - reconstruction_loss: 61.2738 - kl_loss: 4.4261e-05\n",
      "Epoch 100/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.1712 - reconstruction_loss: 61.3231 - kl_loss: 3.9074e-05\n",
      "Epoch 101/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.1746 - reconstruction_loss: 61.2227 - kl_loss: 5.0527e-05\n",
      "Epoch 102/500\n",
      "138/138 [==============================] - 2s 15ms/step - loss: 61.5708 - reconstruction_loss: 61.3044 - kl_loss: 4.4145e-05\n",
      "Epoch 103/500\n",
      "138/138 [==============================] - 2s 15ms/step - loss: 61.0561 - reconstruction_loss: 61.3746 - kl_loss: 3.9190e-05\n",
      "Epoch 104/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.3163 - reconstruction_loss: 61.3334 - kl_loss: 4.4917e-05\n",
      "Epoch 105/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.3481 - reconstruction_loss: 61.2458 - kl_loss: 3.5395e-05\n",
      "Epoch 106/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.1508 - reconstruction_loss: 61.2344 - kl_loss: 4.2773e-05\n",
      "Epoch 107/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.3269 - reconstruction_loss: 61.2450 - kl_loss: 4.6140e-05\n",
      "Epoch 108/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.2892 - reconstruction_loss: 61.2477 - kl_loss: 4.7533e-05\n",
      "Epoch 109/500\n",
      "138/138 [==============================] - 2s 11ms/step - loss: 61.2246 - reconstruction_loss: 61.2639 - kl_loss: 3.5806e-05\n",
      "Epoch 110/500\n",
      "138/138 [==============================] - 2s 11ms/step - loss: 61.2917 - reconstruction_loss: 61.2553 - kl_loss: 4.0839e-05\n",
      "Epoch 111/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.2625 - reconstruction_loss: 61.2274 - kl_loss: 5.3010e-05\n",
      "Epoch 112/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.1045 - reconstruction_loss: 61.2637 - kl_loss: 2.9586e-05\n",
      "Epoch 113/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.2932 - reconstruction_loss: 61.2857 - kl_loss: 5.5167e-05\n",
      "Epoch 114/500\n",
      "138/138 [==============================] - 2s 15ms/step - loss: 61.3718 - reconstruction_loss: 61.4318 - kl_loss: 3.2082e-05\n",
      "Epoch 115/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.2945 - reconstruction_loss: 61.2836 - kl_loss: 6.8745e-05\n",
      "Epoch 116/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.2141 - reconstruction_loss: 61.2670 - kl_loss: 3.0239e-05\n",
      "Epoch 117/500\n",
      "138/138 [==============================] - 2s 15ms/step - loss: 61.3559 - reconstruction_loss: 61.3553 - kl_loss: 3.7688e-05\n",
      "Epoch 118/500\n",
      "138/138 [==============================] - 2s 15ms/step - loss: 61.2046 - reconstruction_loss: 61.3758 - kl_loss: 5.2905e-05\n",
      "Epoch 119/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.3688 - reconstruction_loss: 61.2260 - kl_loss: 3.9176e-05\n",
      "Epoch 120/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.2474 - reconstruction_loss: 61.2517 - kl_loss: 3.4236e-05\n",
      "Epoch 121/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.2090 - reconstruction_loss: 61.2422 - kl_loss: 3.0835e-05\n",
      "Epoch 122/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.2786 - reconstruction_loss: 61.3003 - kl_loss: 3.6408e-05\n",
      "Epoch 123/500\n",
      "138/138 [==============================] - 2s 11ms/step - loss: 61.4026 - reconstruction_loss: 61.2436 - kl_loss: 3.9736e-05\n",
      "Epoch 124/500\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 61.0831 - reconstruction_loss: 61.2960 - kl_loss: 4.0164e-05\n",
      "Epoch 125/500\n",
      "138/138 [==============================] - 2s 11ms/step - loss: 61.3196 - reconstruction_loss: 61.2673 - kl_loss: 2.9087e-05\n",
      "Epoch 126/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.1525 - reconstruction_loss: 61.2274 - kl_loss: 3.7585e-05\n",
      "Epoch 127/500\n",
      "138/138 [==============================] - 2s 15ms/step - loss: 61.1044 - reconstruction_loss: 61.2818 - kl_loss: 3.1310e-05\n",
      "Epoch 128/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.3194 - reconstruction_loss: 61.3100 - kl_loss: 2.5344e-05\n",
      "Epoch 129/500\n",
      "138/138 [==============================] - 2s 16ms/step - loss: 61.4914 - reconstruction_loss: 61.3945 - kl_loss: 2.3357e-05\n",
      "Epoch 130/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.4011 - reconstruction_loss: 61.2822 - kl_loss: 2.5268e-05\n",
      "Epoch 131/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.3191 - reconstruction_loss: 61.2827 - kl_loss: 2.3354e-05\n",
      "Epoch 132/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.3937 - reconstruction_loss: 61.2491 - kl_loss: 2.3613e-05\n",
      "Epoch 133/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.4487 - reconstruction_loss: 61.4919 - kl_loss: 4.6502e-05\n",
      "Epoch 134/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.5056 - reconstruction_loss: 61.2662 - kl_loss: 4.4876e-05\n",
      "Epoch 135/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.1591 - reconstruction_loss: 61.3143 - kl_loss: 3.8214e-05\n",
      "Epoch 136/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.2130 - reconstruction_loss: 61.3525 - kl_loss: 4.4188e-05\n",
      "Epoch 137/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.2133 - reconstruction_loss: 61.2506 - kl_loss: 3.5293e-05\n",
      "Epoch 138/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.4600 - reconstruction_loss: 61.3210 - kl_loss: 4.4649e-05\n",
      "Epoch 139/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.3912 - reconstruction_loss: 61.2416 - kl_loss: 4.1121e-05\n",
      "Epoch 140/500\n",
      "138/138 [==============================] - 2s 15ms/step - loss: 61.3912 - reconstruction_loss: 61.2699 - kl_loss: 3.4268e-05\n",
      "Epoch 141/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.2983 - reconstruction_loss: 61.3343 - kl_loss: 3.8770e-05\n",
      "Epoch 142/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.2274 - reconstruction_loss: 61.3223 - kl_loss: 7.2984e-05\n",
      "Epoch 143/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.3141 - reconstruction_loss: 61.3079 - kl_loss: 3.6386e-05\n",
      "Epoch 144/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.1437 - reconstruction_loss: 61.3029 - kl_loss: 2.8856e-05\n",
      "Epoch 145/500\n",
      "138/138 [==============================] - 2s 15ms/step - loss: 61.4632 - reconstruction_loss: 61.3018 - kl_loss: 4.8810e-05\n",
      "Epoch 146/500\n",
      "138/138 [==============================] - 2s 15ms/step - loss: 61.3797 - reconstruction_loss: 61.3398 - kl_loss: 3.1151e-05\n",
      "Epoch 147/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.1156 - reconstruction_loss: 61.2631 - kl_loss: 2.4585e-05\n",
      "Epoch 148/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.2598 - reconstruction_loss: 61.2198 - kl_loss: 2.5394e-05\n",
      "Epoch 149/500\n",
      "138/138 [==============================] - 2s 15ms/step - loss: 61.3568 - reconstruction_loss: 61.3279 - kl_loss: 2.2153e-05\n",
      "Epoch 150/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.2542 - reconstruction_loss: 61.2562 - kl_loss: 2.1662e-05\n",
      "Epoch 151/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.3032 - reconstruction_loss: 61.2239 - kl_loss: 3.1000e-05\n",
      "Epoch 152/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.2376 - reconstruction_loss: 61.2643 - kl_loss: 2.8241e-05\n",
      "Epoch 153/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.2042 - reconstruction_loss: 61.2904 - kl_loss: 4.3077e-05\n",
      "Epoch 154/500\n",
      "138/138 [==============================] - 2s 11ms/step - loss: 61.0730 - reconstruction_loss: 61.2985 - kl_loss: 3.1135e-05\n",
      "Epoch 155/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.2548 - reconstruction_loss: 61.2866 - kl_loss: 5.2198e-05\n",
      "Epoch 156/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.2994 - reconstruction_loss: 61.2898 - kl_loss: 3.0123e-05\n",
      "Epoch 157/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.3636 - reconstruction_loss: 61.2453 - kl_loss: 2.8211e-05\n",
      "Epoch 158/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.3613 - reconstruction_loss: 61.2902 - kl_loss: 2.2437e-05\n",
      "Epoch 159/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.1514 - reconstruction_loss: 61.2736 - kl_loss: 2.6972e-05\n",
      "Epoch 160/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.0530 - reconstruction_loss: 61.2621 - kl_loss: 3.1457e-05\n",
      "Epoch 161/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.2680 - reconstruction_loss: 61.3331 - kl_loss: 3.8634e-05\n",
      "Epoch 162/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.1574 - reconstruction_loss: 61.3225 - kl_loss: 4.2510e-05\n",
      "Epoch 163/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.3658 - reconstruction_loss: 61.3133 - kl_loss: 5.7737e-05\n",
      "Epoch 164/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.3207 - reconstruction_loss: 61.3133 - kl_loss: 4.0385e-05\n",
      "Epoch 165/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.3590 - reconstruction_loss: 61.3297 - kl_loss: 6.2027e-05\n",
      "Epoch 166/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.2283 - reconstruction_loss: 61.3147 - kl_loss: 4.1842e-05\n",
      "Epoch 167/500\n",
      "138/138 [==============================] - 2s 11ms/step - loss: 61.1865 - reconstruction_loss: 61.3551 - kl_loss: 4.0129e-05\n",
      "Epoch 168/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.4921 - reconstruction_loss: 61.3544 - kl_loss: 3.6886e-05\n",
      "Epoch 169/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.3297 - reconstruction_loss: 61.3062 - kl_loss: 2.8396e-05\n",
      "Epoch 170/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.2702 - reconstruction_loss: 61.2399 - kl_loss: 2.9943e-05\n",
      "Epoch 171/500\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 61.1239 - reconstruction_loss: 61.3236 - kl_loss: 2.0890e-05\n",
      "Epoch 172/500\n",
      "138/138 [==============================] - 2s 15ms/step - loss: 61.2907 - reconstruction_loss: 61.2353 - kl_loss: 2.4753e-05\n",
      "Epoch 173/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.2856 - reconstruction_loss: 61.2685 - kl_loss: 3.2734e-05\n",
      "Epoch 174/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.2952 - reconstruction_loss: 61.3425 - kl_loss: 2.9923e-05\n",
      "Epoch 175/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.3322 - reconstruction_loss: 61.3461 - kl_loss: 3.3213e-05\n",
      "Epoch 176/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.3572 - reconstruction_loss: 61.2982 - kl_loss: 3.8186e-05\n",
      "Epoch 177/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.2936 - reconstruction_loss: 61.2860 - kl_loss: 2.0099e-05\n",
      "Epoch 178/500\n",
      "138/138 [==============================] - 2s 15ms/step - loss: 61.3049 - reconstruction_loss: 61.2820 - kl_loss: 3.2408e-05\n",
      "Epoch 179/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.3610 - reconstruction_loss: 61.2634 - kl_loss: 5.0941e-05\n",
      "Epoch 180/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.5062 - reconstruction_loss: 61.3330 - kl_loss: 2.8679e-05\n",
      "Epoch 181/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.2106 - reconstruction_loss: 61.2172 - kl_loss: 2.2441e-05\n",
      "Epoch 182/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.2172 - reconstruction_loss: 61.3037 - kl_loss: 2.9720e-05\n",
      "Epoch 183/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.3837 - reconstruction_loss: 61.2363 - kl_loss: 2.9655e-05\n",
      "Epoch 184/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.2320 - reconstruction_loss: 61.2462 - kl_loss: 2.5860e-05\n",
      "Epoch 185/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.1043 - reconstruction_loss: 61.2900 - kl_loss: 2.8087e-05\n",
      "Epoch 186/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.1580 - reconstruction_loss: 61.2556 - kl_loss: 5.5175e-05\n",
      "Epoch 187/500\n",
      "138/138 [==============================] - 2s 15ms/step - loss: 61.1142 - reconstruction_loss: 61.2814 - kl_loss: 2.3575e-05\n",
      "Epoch 188/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.2584 - reconstruction_loss: 61.2802 - kl_loss: 2.4908e-05\n",
      "Epoch 189/500\n",
      "138/138 [==============================] - 2s 15ms/step - loss: 61.3885 - reconstruction_loss: 61.2049 - kl_loss: 3.2850e-05\n",
      "Epoch 190/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.3156 - reconstruction_loss: 61.2914 - kl_loss: 2.6096e-05\n",
      "Epoch 191/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.2619 - reconstruction_loss: 61.2884 - kl_loss: 2.5420e-05\n",
      "Epoch 192/500\n",
      "138/138 [==============================] - 2s 11ms/step - loss: 61.2259 - reconstruction_loss: 61.2537 - kl_loss: 3.0433e-05\n",
      "Epoch 193/500\n",
      "138/138 [==============================] - 1s 11ms/step - loss: 61.3810 - reconstruction_loss: 61.3290 - kl_loss: 2.6176e-05\n",
      "Epoch 194/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.4442 - reconstruction_loss: 61.4297 - kl_loss: 2.6480e-05\n",
      "Epoch 195/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.1407 - reconstruction_loss: 61.3355 - kl_loss: 3.1326e-05\n",
      "Epoch 196/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.3022 - reconstruction_loss: 61.2477 - kl_loss: 2.5347e-05\n",
      "Epoch 197/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.3923 - reconstruction_loss: 61.3702 - kl_loss: 2.9467e-05\n",
      "Epoch 198/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.2321 - reconstruction_loss: 61.2774 - kl_loss: 2.7689e-05\n",
      "Epoch 199/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.3127 - reconstruction_loss: 61.2995 - kl_loss: 2.5810e-05\n",
      "Epoch 200/500\n",
      "138/138 [==============================] - 2s 15ms/step - loss: 61.2901 - reconstruction_loss: 61.2958 - kl_loss: 2.1585e-05\n",
      "Epoch 201/500\n",
      "138/138 [==============================] - 2s 15ms/step - loss: 61.2726 - reconstruction_loss: 61.2584 - kl_loss: 2.4950e-05\n",
      "Epoch 202/500\n",
      "138/138 [==============================] - 2s 15ms/step - loss: 61.3922 - reconstruction_loss: 61.2802 - kl_loss: 2.3502e-05\n",
      "Epoch 203/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.1599 - reconstruction_loss: 61.2290 - kl_loss: 1.7497e-05\n",
      "Epoch 204/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.2538 - reconstruction_loss: 61.2346 - kl_loss: 1.9011e-05\n",
      "Epoch 205/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.3567 - reconstruction_loss: 61.2732 - kl_loss: 2.3409e-05\n",
      "Epoch 206/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.3010 - reconstruction_loss: 61.2323 - kl_loss: 2.1706e-05\n",
      "Epoch 207/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.5226 - reconstruction_loss: 61.2538 - kl_loss: 1.8198e-05\n",
      "Epoch 208/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.2389 - reconstruction_loss: 61.3490 - kl_loss: 2.1714e-05\n",
      "Epoch 209/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.1906 - reconstruction_loss: 61.2627 - kl_loss: 2.2883e-05\n",
      "Epoch 210/500\n",
      "138/138 [==============================] - 1s 11ms/step - loss: 61.2406 - reconstruction_loss: 61.4068 - kl_loss: 2.9527e-05\n",
      "Epoch 211/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.1080 - reconstruction_loss: 61.3335 - kl_loss: 2.5351e-05\n",
      "Epoch 212/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.3248 - reconstruction_loss: 61.4366 - kl_loss: 2.7009e-05\n",
      "Epoch 213/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.3988 - reconstruction_loss: 61.2234 - kl_loss: 3.4146e-05\n",
      "Epoch 214/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.2837 - reconstruction_loss: 61.2294 - kl_loss: 2.9552e-05\n",
      "Epoch 215/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.2659 - reconstruction_loss: 61.3285 - kl_loss: 1.7806e-05\n",
      "Epoch 216/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.3276 - reconstruction_loss: 61.2883 - kl_loss: 2.7131e-05\n",
      "Epoch 217/500\n",
      "138/138 [==============================] - 2s 11ms/step - loss: 61.3881 - reconstruction_loss: 61.3314 - kl_loss: 2.0553e-05\n",
      "Epoch 218/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.3362 - reconstruction_loss: 61.2564 - kl_loss: 3.4856e-05\n",
      "Epoch 219/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.1499 - reconstruction_loss: 61.3961 - kl_loss: 3.3105e-05\n",
      "Epoch 220/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.3405 - reconstruction_loss: 61.2752 - kl_loss: 2.9046e-05\n",
      "Epoch 221/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.2770 - reconstruction_loss: 61.2991 - kl_loss: 3.1914e-05\n",
      "Epoch 222/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.2725 - reconstruction_loss: 61.2373 - kl_loss: 2.2078e-05\n",
      "Epoch 223/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.0908 - reconstruction_loss: 61.3207 - kl_loss: 2.2761e-05\n",
      "Epoch 224/500\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 61.3863 - reconstruction_loss: 61.3183 - kl_loss: 2.1620e-05\n",
      "Epoch 225/500\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 61.3653 - reconstruction_loss: 61.2809 - kl_loss: 2.2705e-05\n",
      "Epoch 226/500\n",
      "138/138 [==============================] - 1s 11ms/step - loss: 61.3331 - reconstruction_loss: 61.3184 - kl_loss: 2.7798e-05\n",
      "Epoch 227/500\n",
      "138/138 [==============================] - 2s 11ms/step - loss: 61.2703 - reconstruction_loss: 61.3526 - kl_loss: 3.8209e-05\n",
      "Epoch 228/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.2178 - reconstruction_loss: 61.2773 - kl_loss: 3.0645e-05\n",
      "Epoch 229/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.2832 - reconstruction_loss: 61.3184 - kl_loss: 2.9223e-05\n",
      "Epoch 230/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.1245 - reconstruction_loss: 61.2733 - kl_loss: 2.4967e-05\n",
      "Epoch 231/500\n",
      "138/138 [==============================] - 2s 15ms/step - loss: 61.0762 - reconstruction_loss: 61.3619 - kl_loss: 2.4471e-05\n",
      "Epoch 232/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.4159 - reconstruction_loss: 61.2176 - kl_loss: 3.8693e-05\n",
      "Epoch 233/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.2600 - reconstruction_loss: 61.2751 - kl_loss: 2.3770e-05\n",
      "Epoch 234/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.1758 - reconstruction_loss: 61.2593 - kl_loss: 1.9693e-05\n",
      "Epoch 235/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.2807 - reconstruction_loss: 61.3121 - kl_loss: 2.5451e-05\n",
      "Epoch 236/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.2351 - reconstruction_loss: 61.2557 - kl_loss: 2.5739e-05\n",
      "Epoch 237/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.3284 - reconstruction_loss: 61.3268 - kl_loss: 2.6354e-05\n",
      "Epoch 238/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.1958 - reconstruction_loss: 61.2349 - kl_loss: 2.4644e-05\n",
      "Epoch 239/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.1880 - reconstruction_loss: 61.2556 - kl_loss: 2.2494e-05\n",
      "Epoch 240/500\n",
      "138/138 [==============================] - 2s 11ms/step - loss: 61.4333 - reconstruction_loss: 61.3413 - kl_loss: 2.1038e-05\n",
      "Epoch 241/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.3391 - reconstruction_loss: 61.2720 - kl_loss: 1.8171e-05\n",
      "Epoch 242/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.1390 - reconstruction_loss: 61.2337 - kl_loss: 2.0018e-05\n",
      "Epoch 243/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.1995 - reconstruction_loss: 61.2397 - kl_loss: 2.2214e-05\n",
      "Epoch 244/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.3608 - reconstruction_loss: 61.3264 - kl_loss: 2.1116e-05\n",
      "Epoch 245/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.2926 - reconstruction_loss: 61.2778 - kl_loss: 1.9608e-05\n",
      "Epoch 246/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.1934 - reconstruction_loss: 61.2096 - kl_loss: 2.5977e-05\n",
      "Epoch 247/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.4899 - reconstruction_loss: 61.3441 - kl_loss: 2.7354e-05\n",
      "Epoch 248/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.3980 - reconstruction_loss: 61.2921 - kl_loss: 2.5455e-05\n",
      "Epoch 249/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.3068 - reconstruction_loss: 61.3274 - kl_loss: 2.2974e-05\n",
      "Epoch 250/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.3292 - reconstruction_loss: 61.2789 - kl_loss: 2.5660e-05\n",
      "Epoch 251/500\n",
      "138/138 [==============================] - 2s 15ms/step - loss: 61.2273 - reconstruction_loss: 61.3156 - kl_loss: 2.7783e-05\n",
      "Epoch 252/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.4761 - reconstruction_loss: 61.3764 - kl_loss: 2.4829e-05\n",
      "Epoch 253/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.4597 - reconstruction_loss: 61.2832 - kl_loss: 2.9318e-05\n",
      "Epoch 254/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.3331 - reconstruction_loss: 61.2122 - kl_loss: 3.2962e-05\n",
      "Epoch 255/500\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 61.2093 - reconstruction_loss: 61.2646 - kl_loss: 2.2699e-05\n",
      "Epoch 256/500\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 61.3105 - reconstruction_loss: 61.3149 - kl_loss: 2.4663e-05\n",
      "Epoch 257/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.3393 - reconstruction_loss: 61.2783 - kl_loss: 2.2479e-05\n",
      "Epoch 258/500\n",
      "138/138 [==============================] - 2s 11ms/step - loss: 61.1804 - reconstruction_loss: 61.2245 - kl_loss: 1.8644e-05\n",
      "Epoch 259/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.2482 - reconstruction_loss: 61.2621 - kl_loss: 1.8719e-05\n",
      "Epoch 260/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.1627 - reconstruction_loss: 61.2784 - kl_loss: 2.5404e-05\n",
      "Epoch 261/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.2482 - reconstruction_loss: 61.3306 - kl_loss: 1.8093e-05\n",
      "Epoch 262/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.2132 - reconstruction_loss: 61.2534 - kl_loss: 2.6428e-05\n",
      "Epoch 263/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.2561 - reconstruction_loss: 61.2811 - kl_loss: 2.4849e-05\n",
      "Epoch 264/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.2050 - reconstruction_loss: 61.2846 - kl_loss: 2.6117e-05\n",
      "Epoch 265/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 60.9933 - reconstruction_loss: 61.2504 - kl_loss: 2.4566e-05\n",
      "Epoch 266/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.2452 - reconstruction_loss: 61.2925 - kl_loss: 2.6709e-05\n",
      "Epoch 267/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.3550 - reconstruction_loss: 61.3131 - kl_loss: 2.5397e-05\n",
      "Epoch 268/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.0311 - reconstruction_loss: 61.3289 - kl_loss: 1.9223e-05\n",
      "Epoch 269/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.2056 - reconstruction_loss: 61.3032 - kl_loss: 2.6552e-05\n",
      "Epoch 270/500\n",
      "138/138 [==============================] - 2s 11ms/step - loss: 61.3844 - reconstruction_loss: 61.2363 - kl_loss: 2.1779e-05\n",
      "Epoch 271/500\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 61.3227 - reconstruction_loss: 61.2279 - kl_loss: 2.6138e-05\n",
      "Epoch 272/500\n",
      "138/138 [==============================] - 1s 11ms/step - loss: 61.4387 - reconstruction_loss: 61.2905 - kl_loss: 2.1557e-05\n",
      "Epoch 273/500\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 61.3232 - reconstruction_loss: 61.2211 - kl_loss: 2.5117e-05\n",
      "Epoch 274/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.3386 - reconstruction_loss: 61.3131 - kl_loss: 2.6208e-05\n",
      "Epoch 275/500\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 61.2125 - reconstruction_loss: 61.2905 - kl_loss: 2.2023e-05\n",
      "Epoch 276/500\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 61.3316 - reconstruction_loss: 61.3417 - kl_loss: 3.5978e-05\n",
      "Epoch 277/500\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 61.2876 - reconstruction_loss: 61.2467 - kl_loss: 2.5996e-05\n",
      "Epoch 278/500\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 61.3546 - reconstruction_loss: 61.3053 - kl_loss: 2.6204e-05\n",
      "Epoch 279/500\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 61.0441 - reconstruction_loss: 61.2650 - kl_loss: 1.7646e-05\n",
      "Epoch 280/500\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 61.2647 - reconstruction_loss: 61.3047 - kl_loss: 2.3191e-05\n",
      "Epoch 281/500\n",
      "138/138 [==============================] - 2s 11ms/step - loss: 61.3145 - reconstruction_loss: 61.2731 - kl_loss: 1.6037e-05\n",
      "Epoch 282/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.2540 - reconstruction_loss: 61.3312 - kl_loss: 1.8558e-05\n",
      "Epoch 283/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.3569 - reconstruction_loss: 61.2313 - kl_loss: 1.9647e-05\n",
      "Epoch 284/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.2410 - reconstruction_loss: 61.3765 - kl_loss: 2.1358e-05\n",
      "Epoch 285/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.4048 - reconstruction_loss: 61.3222 - kl_loss: 2.0779e-05\n",
      "Epoch 286/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.2829 - reconstruction_loss: 61.3797 - kl_loss: 2.2553e-05\n",
      "Epoch 287/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.2793 - reconstruction_loss: 61.2798 - kl_loss: 2.4985e-05\n",
      "Epoch 288/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.2899 - reconstruction_loss: 61.2503 - kl_loss: 2.4017e-05\n",
      "Epoch 289/500\n",
      "138/138 [==============================] - 2s 11ms/step - loss: 61.2847 - reconstruction_loss: 61.3684 - kl_loss: 2.0828e-05\n",
      "Epoch 290/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.3638 - reconstruction_loss: 61.2378 - kl_loss: 2.6984e-05\n",
      "Epoch 291/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.4280 - reconstruction_loss: 61.2813 - kl_loss: 1.8817e-05\n",
      "Epoch 292/500\n",
      "138/138 [==============================] - 1s 11ms/step - loss: 61.3739 - reconstruction_loss: 61.2711 - kl_loss: 2.0699e-05\n",
      "Epoch 293/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.3013 - reconstruction_loss: 61.2909 - kl_loss: 1.9312e-05\n",
      "Epoch 294/500\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 61.3617 - reconstruction_loss: 61.3348 - kl_loss: 2.1553e-05\n",
      "Epoch 295/500\n",
      "138/138 [==============================] - 2s 11ms/step - loss: 61.3199 - reconstruction_loss: 61.2647 - kl_loss: 2.6552e-05\n",
      "Epoch 296/500\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 61.1091 - reconstruction_loss: 61.4058 - kl_loss: 2.6545e-05\n",
      "Epoch 297/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.3345 - reconstruction_loss: 61.3275 - kl_loss: 2.5697e-05\n",
      "Epoch 298/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 60.9990 - reconstruction_loss: 61.2714 - kl_loss: 2.0296e-05\n",
      "Epoch 299/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.1670 - reconstruction_loss: 61.2401 - kl_loss: 2.3558e-05\n",
      "Epoch 300/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.2432 - reconstruction_loss: 61.2564 - kl_loss: 2.2440e-05\n",
      "Epoch 301/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.2609 - reconstruction_loss: 61.3386 - kl_loss: 2.0743e-05\n",
      "Epoch 302/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.3550 - reconstruction_loss: 61.2781 - kl_loss: 2.4022e-05\n",
      "Epoch 303/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.1413 - reconstruction_loss: 61.2775 - kl_loss: 2.3991e-05\n",
      "Epoch 304/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.2240 - reconstruction_loss: 61.2395 - kl_loss: 1.8644e-05\n",
      "Epoch 305/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.2357 - reconstruction_loss: 61.2605 - kl_loss: 2.2558e-05\n",
      "Epoch 306/500\n",
      "138/138 [==============================] - 2s 15ms/step - loss: 61.2887 - reconstruction_loss: 61.2817 - kl_loss: 2.0466e-05\n",
      "Epoch 307/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.2952 - reconstruction_loss: 61.3226 - kl_loss: 1.9933e-05\n",
      "Epoch 308/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.2836 - reconstruction_loss: 61.2003 - kl_loss: 2.0479e-05\n",
      "Epoch 309/500\n",
      "138/138 [==============================] - 2s 11ms/step - loss: 61.1635 - reconstruction_loss: 61.2674 - kl_loss: 2.3240e-05\n",
      "Epoch 310/500\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 61.3526 - reconstruction_loss: 61.3138 - kl_loss: 1.6729e-05\n",
      "Epoch 311/500\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 61.2151 - reconstruction_loss: 61.2957 - kl_loss: 1.8372e-05\n",
      "Epoch 312/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.3757 - reconstruction_loss: 61.2961 - kl_loss: 1.7099e-05\n",
      "Epoch 313/500\n",
      "138/138 [==============================] - 1s 11ms/step - loss: 61.2989 - reconstruction_loss: 61.3471 - kl_loss: 2.3462e-05\n",
      "Epoch 314/500\n",
      "138/138 [==============================] - 1s 11ms/step - loss: 61.4383 - reconstruction_loss: 61.3522 - kl_loss: 1.9254e-05\n",
      "Epoch 315/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.2290 - reconstruction_loss: 61.2521 - kl_loss: 3.3644e-05\n",
      "Epoch 316/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.3358 - reconstruction_loss: 61.2458 - kl_loss: 2.5571e-05\n",
      "Epoch 317/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.1100 - reconstruction_loss: 61.2867 - kl_loss: 2.5534e-05\n",
      "Epoch 318/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.3095 - reconstruction_loss: 61.2652 - kl_loss: 2.1955e-05\n",
      "Epoch 319/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.4232 - reconstruction_loss: 61.3576 - kl_loss: 2.1670e-05\n",
      "Epoch 320/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.2353 - reconstruction_loss: 61.2530 - kl_loss: 4.0241e-05\n",
      "Epoch 321/500\n",
      "138/138 [==============================] - 2s 11ms/step - loss: 61.2340 - reconstruction_loss: 61.2585 - kl_loss: 2.0976e-05\n",
      "Epoch 322/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.4932 - reconstruction_loss: 61.2968 - kl_loss: 1.8327e-05\n",
      "Epoch 323/500\n",
      "138/138 [==============================] - 2s 15ms/step - loss: 61.4297 - reconstruction_loss: 61.2891 - kl_loss: 1.8525e-05\n",
      "Epoch 324/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.1763 - reconstruction_loss: 61.2560 - kl_loss: 2.5785e-05\n",
      "Epoch 325/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.3865 - reconstruction_loss: 61.3371 - kl_loss: 1.7958e-05\n",
      "Epoch 326/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.2361 - reconstruction_loss: 61.3057 - kl_loss: 1.6156e-05\n",
      "Epoch 327/500\n",
      "138/138 [==============================] - 2s 11ms/step - loss: 61.2118 - reconstruction_loss: 61.4373 - kl_loss: 2.2395e-05\n",
      "Epoch 328/500\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 61.4463 - reconstruction_loss: 61.2608 - kl_loss: 1.8972e-05\n",
      "Epoch 329/500\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 61.0683 - reconstruction_loss: 61.2884 - kl_loss: 1.6092e-05\n",
      "Epoch 330/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.1416 - reconstruction_loss: 61.3468 - kl_loss: 2.1270e-05\n",
      "Epoch 331/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.1893 - reconstruction_loss: 61.3006 - kl_loss: 2.1521e-05\n",
      "Epoch 332/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.4334 - reconstruction_loss: 61.3166 - kl_loss: 3.1000e-05\n",
      "Epoch 333/500\n",
      "138/138 [==============================] - 2s 11ms/step - loss: 61.1777 - reconstruction_loss: 61.2519 - kl_loss: 2.2354e-05\n",
      "Epoch 334/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.4824 - reconstruction_loss: 61.2738 - kl_loss: 1.9595e-05\n",
      "Epoch 335/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.3272 - reconstruction_loss: 61.2886 - kl_loss: 2.4599e-05\n",
      "Epoch 336/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.0823 - reconstruction_loss: 61.3949 - kl_loss: 2.2756e-05\n",
      "Epoch 337/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.1575 - reconstruction_loss: 61.2809 - kl_loss: 2.2732e-05\n",
      "Epoch 338/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.2460 - reconstruction_loss: 61.3426 - kl_loss: 2.0651e-05\n",
      "Epoch 339/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.3194 - reconstruction_loss: 61.2708 - kl_loss: 2.9995e-05\n",
      "Epoch 340/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.3365 - reconstruction_loss: 61.2970 - kl_loss: 2.0519e-05\n",
      "Epoch 341/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.3284 - reconstruction_loss: 61.3793 - kl_loss: 2.7709e-05\n",
      "Epoch 342/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.2335 - reconstruction_loss: 61.2437 - kl_loss: 3.7548e-05\n",
      "Epoch 343/500\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 61.2941 - reconstruction_loss: 61.3151 - kl_loss: 1.7021e-05\n",
      "Epoch 344/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.1765 - reconstruction_loss: 61.3136 - kl_loss: 2.5103e-05\n",
      "Epoch 345/500\n",
      "138/138 [==============================] - 2s 11ms/step - loss: 61.2966 - reconstruction_loss: 61.2876 - kl_loss: 2.3496e-05\n",
      "Epoch 346/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.3600 - reconstruction_loss: 61.2781 - kl_loss: 2.4669e-05\n",
      "Epoch 347/500\n",
      "138/138 [==============================] - 1s 11ms/step - loss: 61.3333 - reconstruction_loss: 61.3793 - kl_loss: 2.5580e-05\n",
      "Epoch 348/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.3960 - reconstruction_loss: 61.3806 - kl_loss: 3.5965e-05\n",
      "Epoch 349/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.2857 - reconstruction_loss: 61.2329 - kl_loss: 2.2868e-05\n",
      "Epoch 350/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.5328 - reconstruction_loss: 61.2332 - kl_loss: 2.2610e-05\n",
      "Epoch 351/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.2488 - reconstruction_loss: 61.3258 - kl_loss: 1.6362e-05\n",
      "Epoch 352/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.2928 - reconstruction_loss: 61.2646 - kl_loss: 2.1809e-05\n",
      "Epoch 353/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.0615 - reconstruction_loss: 61.3269 - kl_loss: 1.8521e-05\n",
      "Epoch 354/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.2968 - reconstruction_loss: 61.3785 - kl_loss: 1.8540e-05\n",
      "Epoch 355/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.3286 - reconstruction_loss: 61.4012 - kl_loss: 2.4818e-05\n",
      "Epoch 356/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.3661 - reconstruction_loss: 61.2742 - kl_loss: 1.8335e-05\n",
      "Epoch 357/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.2370 - reconstruction_loss: 61.3160 - kl_loss: 2.0694e-05\n",
      "Epoch 358/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.4518 - reconstruction_loss: 61.2147 - kl_loss: 1.9062e-05\n",
      "Epoch 359/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.2562 - reconstruction_loss: 61.3008 - kl_loss: 1.5255e-05\n",
      "Epoch 360/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.3140 - reconstruction_loss: 61.2431 - kl_loss: 1.7640e-05\n",
      "Epoch 361/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.1813 - reconstruction_loss: 61.2582 - kl_loss: 1.3481e-05\n",
      "Epoch 362/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.2383 - reconstruction_loss: 61.3184 - kl_loss: 2.0651e-05\n",
      "Epoch 363/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.2341 - reconstruction_loss: 61.3264 - kl_loss: 1.9551e-05\n",
      "Epoch 364/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.4421 - reconstruction_loss: 61.3077 - kl_loss: 3.1094e-05\n",
      "Epoch 365/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.2093 - reconstruction_loss: 61.2999 - kl_loss: 2.1035e-05\n",
      "Epoch 366/500\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 61.1382 - reconstruction_loss: 61.2215 - kl_loss: 1.8492e-05\n",
      "Epoch 367/500\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 61.4424 - reconstruction_loss: 61.2798 - kl_loss: 2.0729e-05\n",
      "Epoch 368/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.0655 - reconstruction_loss: 61.2823 - kl_loss: 1.8211e-05\n",
      "Epoch 369/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.3410 - reconstruction_loss: 61.3368 - kl_loss: 2.4052e-05\n",
      "Epoch 370/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.2798 - reconstruction_loss: 61.2388 - kl_loss: 2.3792e-05\n",
      "Epoch 371/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.2385 - reconstruction_loss: 61.2692 - kl_loss: 2.1138e-05\n",
      "Epoch 372/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.3446 - reconstruction_loss: 61.2436 - kl_loss: 1.6081e-05\n",
      "Epoch 373/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.2764 - reconstruction_loss: 61.2401 - kl_loss: 1.7024e-05\n",
      "Epoch 374/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.3182 - reconstruction_loss: 61.2539 - kl_loss: 1.5535e-05\n",
      "Epoch 375/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.2683 - reconstruction_loss: 61.3048 - kl_loss: 2.2384e-05\n",
      "Epoch 376/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.3189 - reconstruction_loss: 61.2640 - kl_loss: 1.8474e-05\n",
      "Epoch 377/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.1660 - reconstruction_loss: 61.2558 - kl_loss: 1.7659e-05\n",
      "Epoch 378/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 60.9775 - reconstruction_loss: 61.2273 - kl_loss: 1.9904e-05\n",
      "Epoch 379/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.5163 - reconstruction_loss: 61.3713 - kl_loss: 1.6404e-05\n",
      "Epoch 380/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.4674 - reconstruction_loss: 61.3054 - kl_loss: 2.3857e-05\n",
      "Epoch 381/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.4367 - reconstruction_loss: 61.3439 - kl_loss: 1.9239e-05\n",
      "Epoch 382/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.2519 - reconstruction_loss: 61.3087 - kl_loss: 2.1244e-05\n",
      "Epoch 383/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.0449 - reconstruction_loss: 61.2893 - kl_loss: 2.2106e-05\n",
      "Epoch 384/500\n",
      "138/138 [==============================] - 2s 11ms/step - loss: 61.1728 - reconstruction_loss: 61.3508 - kl_loss: 2.9577e-05\n",
      "Epoch 385/500\n",
      "138/138 [==============================] - 2s 11ms/step - loss: 61.2536 - reconstruction_loss: 61.3532 - kl_loss: 2.9842e-05\n",
      "Epoch 386/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.2919 - reconstruction_loss: 61.2786 - kl_loss: 2.3375e-05\n",
      "Epoch 387/500\n",
      "138/138 [==============================] - 2s 11ms/step - loss: 61.0910 - reconstruction_loss: 61.3444 - kl_loss: 2.4209e-05\n",
      "Epoch 388/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.3579 - reconstruction_loss: 61.3101 - kl_loss: 2.0337e-05\n",
      "Epoch 389/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.4427 - reconstruction_loss: 61.2714 - kl_loss: 2.1097e-05\n",
      "Epoch 390/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.1527 - reconstruction_loss: 61.2593 - kl_loss: 2.1615e-05\n",
      "Epoch 391/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.2759 - reconstruction_loss: 61.2821 - kl_loss: 2.6151e-05\n",
      "Epoch 392/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.5013 - reconstruction_loss: 61.2703 - kl_loss: 1.8110e-05\n",
      "Epoch 393/500\n",
      "138/138 [==============================] - 1s 11ms/step - loss: 61.3858 - reconstruction_loss: 61.3473 - kl_loss: 1.6499e-05\n",
      "Epoch 394/500\n",
      "138/138 [==============================] - 2s 11ms/step - loss: 61.3508 - reconstruction_loss: 61.2759 - kl_loss: 1.9125e-05\n",
      "Epoch 395/500\n",
      "138/138 [==============================] - 2s 11ms/step - loss: 61.2879 - reconstruction_loss: 61.2342 - kl_loss: 1.9655e-05\n",
      "Epoch 396/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.2134 - reconstruction_loss: 61.3128 - kl_loss: 1.8393e-05\n",
      "Epoch 397/500\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 61.3227 - reconstruction_loss: 61.3317 - kl_loss: 2.1224e-05\n",
      "Epoch 398/500\n",
      "138/138 [==============================] - 2s 11ms/step - loss: 61.2728 - reconstruction_loss: 61.3247 - kl_loss: 1.9092e-05\n",
      "Epoch 399/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.1406 - reconstruction_loss: 61.2243 - kl_loss: 2.3015e-05\n",
      "Epoch 400/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.1913 - reconstruction_loss: 61.3491 - kl_loss: 1.6682e-05\n",
      "Epoch 401/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.3328 - reconstruction_loss: 61.2805 - kl_loss: 1.6560e-05\n",
      "Epoch 402/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.2862 - reconstruction_loss: 61.2487 - kl_loss: 2.5421e-05\n",
      "Epoch 403/500\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 61.2911 - reconstruction_loss: 61.2582 - kl_loss: 2.5687e-05\n",
      "Epoch 404/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.3059 - reconstruction_loss: 61.2554 - kl_loss: 1.6760e-05\n",
      "Epoch 405/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 60.9912 - reconstruction_loss: 61.2687 - kl_loss: 2.0250e-05\n",
      "Epoch 406/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.3355 - reconstruction_loss: 61.2263 - kl_loss: 1.5809e-05\n",
      "Epoch 407/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.3515 - reconstruction_loss: 61.2110 - kl_loss: 1.8048e-05\n",
      "Epoch 408/500\n",
      "138/138 [==============================] - 2s 11ms/step - loss: 61.2010 - reconstruction_loss: 61.2767 - kl_loss: 1.7725e-05\n",
      "Epoch 409/500\n",
      "138/138 [==============================] - 2s 11ms/step - loss: 61.4552 - reconstruction_loss: 61.2875 - kl_loss: 2.7160e-05\n",
      "Epoch 410/500\n",
      "138/138 [==============================] - 2s 11ms/step - loss: 61.3765 - reconstruction_loss: 61.2615 - kl_loss: 3.9155e-05\n",
      "Epoch 411/500\n",
      "138/138 [==============================] - 2s 11ms/step - loss: 61.3397 - reconstruction_loss: 61.3001 - kl_loss: 1.9710e-05\n",
      "Epoch 412/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 60.9131 - reconstruction_loss: 61.2080 - kl_loss: 2.3375e-05\n",
      "Epoch 413/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.3733 - reconstruction_loss: 61.3344 - kl_loss: 1.9332e-05\n",
      "Epoch 414/500\n",
      "138/138 [==============================] - 1s 11ms/step - loss: 61.2954 - reconstruction_loss: 61.2170 - kl_loss: 2.4988e-05\n",
      "Epoch 415/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.2133 - reconstruction_loss: 61.2809 - kl_loss: 2.3376e-05\n",
      "Epoch 416/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.2388 - reconstruction_loss: 61.2316 - kl_loss: 1.3593e-05\n",
      "Epoch 417/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.2843 - reconstruction_loss: 61.3585 - kl_loss: 1.9776e-05\n",
      "Epoch 418/500\n",
      "138/138 [==============================] - 2s 15ms/step - loss: 61.1084 - reconstruction_loss: 61.2515 - kl_loss: 2.3056e-05\n",
      "Epoch 419/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.4095 - reconstruction_loss: 61.2607 - kl_loss: 1.4219e-05\n",
      "Epoch 420/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.3059 - reconstruction_loss: 61.2640 - kl_loss: 1.5910e-05\n",
      "Epoch 421/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.4760 - reconstruction_loss: 61.2543 - kl_loss: 1.8500e-05\n",
      "Epoch 422/500\n",
      "138/138 [==============================] - 2s 11ms/step - loss: 61.4692 - reconstruction_loss: 61.3007 - kl_loss: 1.5793e-05\n",
      "Epoch 423/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.1082 - reconstruction_loss: 61.2674 - kl_loss: 2.5212e-05\n",
      "Epoch 424/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.2537 - reconstruction_loss: 61.3136 - kl_loss: 2.4339e-05\n",
      "Epoch 425/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.1690 - reconstruction_loss: 61.2882 - kl_loss: 2.2591e-05\n",
      "Epoch 426/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.1369 - reconstruction_loss: 61.2813 - kl_loss: 2.9039e-05\n",
      "Epoch 427/500\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 61.1594 - reconstruction_loss: 61.2136 - kl_loss: 1.9415e-05\n",
      "Epoch 428/500\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 61.3137 - reconstruction_loss: 61.3874 - kl_loss: 1.7562e-05\n",
      "Epoch 429/500\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 61.2489 - reconstruction_loss: 61.3959 - kl_loss: 2.4980e-05\n",
      "Epoch 430/500\n",
      "138/138 [==============================] - 2s 11ms/step - loss: 61.3454 - reconstruction_loss: 61.3440 - kl_loss: 2.5873e-05\n",
      "Epoch 431/500\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 61.3842 - reconstruction_loss: 61.3014 - kl_loss: 1.7640e-05\n",
      "Epoch 432/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.3694 - reconstruction_loss: 61.2368 - kl_loss: 1.9264e-05\n",
      "Epoch 433/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.4484 - reconstruction_loss: 61.2660 - kl_loss: 1.3089e-05\n",
      "Epoch 434/500\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 61.1747 - reconstruction_loss: 61.3210 - kl_loss: 1.7827e-05\n",
      "Epoch 435/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.3157 - reconstruction_loss: 61.3013 - kl_loss: 1.9069e-05\n",
      "Epoch 436/500\n",
      "138/138 [==============================] - 1s 11ms/step - loss: 61.4000 - reconstruction_loss: 61.2787 - kl_loss: 2.2269e-05\n",
      "Epoch 437/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.4530 - reconstruction_loss: 61.2474 - kl_loss: 1.5114e-05\n",
      "Epoch 438/500\n",
      "138/138 [==============================] - 2s 15ms/step - loss: 61.1122 - reconstruction_loss: 61.3261 - kl_loss: 1.7302e-05\n",
      "Epoch 439/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.2405 - reconstruction_loss: 61.2537 - kl_loss: 2.6593e-05\n",
      "Epoch 440/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.4089 - reconstruction_loss: 61.2423 - kl_loss: 1.4449e-05\n",
      "Epoch 441/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.4040 - reconstruction_loss: 61.2645 - kl_loss: 2.0787e-05\n",
      "Epoch 442/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.2500 - reconstruction_loss: 61.2706 - kl_loss: 1.3604e-05\n",
      "Epoch 443/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.2377 - reconstruction_loss: 61.3207 - kl_loss: 1.3227e-05\n",
      "Epoch 444/500\n",
      "138/138 [==============================] - 2s 15ms/step - loss: 61.4067 - reconstruction_loss: 61.2422 - kl_loss: 1.3836e-05\n",
      "Epoch 445/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.3352 - reconstruction_loss: 61.2990 - kl_loss: 1.6612e-05\n",
      "Epoch 446/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.1756 - reconstruction_loss: 61.3180 - kl_loss: 2.0310e-05\n",
      "Epoch 447/500\n",
      "138/138 [==============================] - 2s 11ms/step - loss: 61.3064 - reconstruction_loss: 61.2606 - kl_loss: 2.1203e-05\n",
      "Epoch 448/500\n",
      "138/138 [==============================] - 1s 11ms/step - loss: 61.2882 - reconstruction_loss: 61.2796 - kl_loss: 1.6848e-05\n",
      "Epoch 449/500\n",
      "138/138 [==============================] - 2s 11ms/step - loss: 61.3334 - reconstruction_loss: 61.2827 - kl_loss: 1.6740e-05\n",
      "Epoch 450/500\n",
      "138/138 [==============================] - 1s 11ms/step - loss: 61.5423 - reconstruction_loss: 61.3190 - kl_loss: 2.2922e-05\n",
      "Epoch 451/500\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 61.3186 - reconstruction_loss: 61.2249 - kl_loss: 1.7215e-05\n",
      "Epoch 452/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.3217 - reconstruction_loss: 61.2937 - kl_loss: 1.6895e-05\n",
      "Epoch 453/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.4129 - reconstruction_loss: 61.2162 - kl_loss: 1.4385e-05\n",
      "Epoch 454/500\n",
      "138/138 [==============================] - 2s 15ms/step - loss: 61.3955 - reconstruction_loss: 61.2755 - kl_loss: 1.1684e-05\n",
      "Epoch 455/500\n",
      "138/138 [==============================] - 1s 11ms/step - loss: 61.5248 - reconstruction_loss: 61.2307 - kl_loss: 1.6766e-05\n",
      "Epoch 456/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.3709 - reconstruction_loss: 61.3982 - kl_loss: 1.8850e-05\n",
      "Epoch 457/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.3027 - reconstruction_loss: 61.3077 - kl_loss: 2.6362e-05\n",
      "Epoch 458/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.2468 - reconstruction_loss: 61.3526 - kl_loss: 2.4318e-05\n",
      "Epoch 459/500\n",
      "138/138 [==============================] - 2s 14ms/step - loss: 61.1463 - reconstruction_loss: 61.3416 - kl_loss: 2.3583e-05\n",
      "Epoch 460/500\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 61.3866 - reconstruction_loss: 61.2309 - kl_loss: 2.3487e-05\n",
      "Epoch 461/500\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 61.2419 - reconstruction_loss: 61.2232 - kl_loss: 1.8902e-05\n",
      "Epoch 462/500\n",
      "106/138 [======================>.......] - ETA: 0s - loss: 61.3247 - reconstruction_loss: 61.3653 - kl_loss: 1.7259e-05"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m vae \u001b[39m=\u001b[39m VAE(encoder, decoder)\n\u001b[1;32m      2\u001b[0m vae\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39mkeras\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mAdam())\n\u001b[0;32m----> 3\u001b[0m vae\u001b[39m.\u001b[39;49mfit(data, epochs\u001b[39m=\u001b[39;49m\u001b[39m500\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m128\u001b[39;49m)\n",
      "File \u001b[0;32m~/github/VT2_airspace_complexity/.venv/lib/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/github/VT2_airspace_complexity/.venv/lib/python3.9/site-packages/keras/engine/training.py:1650\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1642\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1643\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1644\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1648\u001b[0m ):\n\u001b[1;32m   1649\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1650\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1651\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1652\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/github/VT2_airspace_complexity/.venv/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/github/VT2_airspace_complexity/.venv/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    877\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    879\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 880\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    882\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    883\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/github/VT2_airspace_complexity/.venv/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:912\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    909\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    910\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    911\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 912\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    914\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    915\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    916\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/github/VT2_airspace_complexity/.venv/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    132\u001b[0m   (concrete_function,\n\u001b[1;32m    133\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m    135\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/github/VT2_airspace_complexity/.venv/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1741\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1742\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1743\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1744\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1745\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1746\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1747\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m     args,\n\u001b[1;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1750\u001b[0m     executing_eagerly)\n\u001b[1;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/github/VT2_airspace_complexity/.venv/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    377\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 378\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    379\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    380\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    381\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    382\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    383\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    384\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    386\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    387\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    390\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    391\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/github/VT2_airspace_complexity/.venv/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vae = VAE(encoder, decoder)\n",
    "vae.compile(optimizer=keras.optimizers.Adam())\n",
    "vae.fit(data, epochs=500, batch_size=128)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
