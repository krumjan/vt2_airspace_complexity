{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classes import airspace\n",
    "\n",
    "as_switzerland = airspace(id = 'rectangle_Switzerland',\n",
    "                          volume=(45.52084813133491, \n",
    "                                  47.85, \n",
    "                                  5.85392, \n",
    "                                  10.80539976710652, \n",
    "                                  18000, \n",
    "                                  60000,),\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "as_switzerland.generate_cells()\n",
    "as_switzerland.visualise_cells()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "as_switzerland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "as_switzerland.cubes[0].visualise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = \"2019-01-12 hh:mm:ss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[5:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "as_switzerland.generate_cells()\n",
    "as_switzerland.visualise_cells()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "as_switzerland.cubes[0].visualise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from traffic.core import Traffic\n",
    "trajs = Traffic.from_file(f\"data/rectangle_Switzerland/02_combined/combined_2019_12.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=UserWarning)\n",
    "\n",
    "trajs_resamp =(\n",
    "    trajs.clean_invalid()\n",
    "    .assign_id()\n",
    "    .filter()\n",
    "    .resample(\"5s\")\n",
    "    .eval(max_workers= 20, desc='resampling')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajs.data.head().dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from traffic.core import Traffic\n",
    "x = trajs.data.dtypes.to_dict()\n",
    "del x['last_position']\n",
    "new = trajs_resamp.data.astype(x)\n",
    "trajs_prep = Traffic(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajs_prep = Traffic(new)\n",
    "trajs_prep.data.head().dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "save_path = f\"data/rectangle_Switzerland/03_preprocessed\"\n",
    "if os.path.isdir(save_path) is False:\n",
    "    os.mkdir(save_path)\n",
    "trajs_prep.to_parquet(f\"{save_path}/preprocessed.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from cartes.crs import CH1903p\n",
    "\n",
    "t_gmm = t_unwrapped.clustering(\n",
    "    nb_samples=15,\n",
    "    projection=CH1903p(),\n",
    "    features=[\"x\", \"y\", \"track_unwrapped\"],\n",
    "    clustering=GaussianMixture(n_components=30),\n",
    "    transform=StandardScaler(),\n",
    ").fit_predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(t_gmm.groupby([\"cluster\"]).agg({\"flight_id\": \"nunique\"}).flight_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 1 + t_gmm.data.cluster.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipyleaflet import Map, basemaps, MeasureControl, Polyline\n",
    "from ipywidgets import Layout\n",
    "from traffic.data import nm_airspaces\n",
    "\n",
    "map_ = Map(\n",
    "    center=(47.48891, 8.85743),\n",
    "    zoom=8,\n",
    "    basemap=basemaps.CartoDB.Positron,\n",
    "    layout=Layout(width=\"1000px\", height=\"1000px\"),\n",
    ")\n",
    "\n",
    "# get list of length x with unique colors\n",
    "color_cycle = fn.generate_color_list(n_clusters)\n",
    "\n",
    "for cluster in range(n_clusters):\n",
    "    for flight in t_gmm.query(f\"cluster == {cluster}\"):\n",
    "        map_.add_layer(flight, weight=1, color = color_cycle[cluster], opacity=0.3)\n",
    "map_.add_layer(nm_airspaces[\"LSAZM7\"], weight=2, color='black', fill = False)\n",
    "map_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from traffic.core.projection import EuroPP\n",
    "\n",
    "t = (\n",
    "    t_gmm.query(f\"cluster == {5}\")\n",
    "    .assign_id()\n",
    "    .unwrap()\n",
    "    .resample(100)\n",
    "    .eval()\n",
    ")\n",
    "\n",
    "t = t.compute_xy(projection=EuroPP())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from traffic.core import Traffic\n",
    "\n",
    "def compute_timedelta(df: \"pd.DataFrame\"):\n",
    "    return (df.timestamp - df.timestamp.min()).dt.total_seconds()\n",
    "\n",
    "t = t.iterate_lazy().assign(timedelta=compute_timedelta).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = t.data\n",
    "df_new = df[np.isfinite(df[[\"x\", \"y\", \"altitude\", \"timedelta\"]]).all(1)]\n",
    "df_new = df_new.drop_duplicates(subset=\"timestamp\", keep=\"first\")\n",
    "t = Traffic(df_new)\n",
    "\n",
    "t = (\n",
    "    t\n",
    "    .assign_id()\n",
    "    .unwrap()\n",
    "    .resample(100)\n",
    "    .eval()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from traffic.algorithms.generation import Generation\n",
    "\n",
    "g1 = Generation(\n",
    "    generation=GaussianMixture(n_components=4),\n",
    "    features=[\"x\", \"y\", \"altitude\", \"timedelta\"],\n",
    "    scaler=MinMaxScaler(feature_range=(-1, 1))\n",
    ").fit(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_gen1 = g1.sample(200, projection=EuroPP())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = (\n",
    "#     t\n",
    "#     .drop_duplicates(subset=\"timestamp\", keep=\"first\")\n",
    "#     .resample(200)\n",
    "#     .eval(max_workers=2)\n",
    "# )\n",
    "# t_gen1 = (\n",
    "#     t_gen1\n",
    "#     .drop_duplicates(subset=\"timestamp\", keep=\"first\")\n",
    "#     .resample(200)\n",
    "#     .eval(max_workers=2)\n",
    "# )\n",
    "# t_gen1 = t_gen1.clip(nm_airspaces[\"LSAZM7\"].shape.bounds).eval(desc=\"clipping\", max_workers=2)\n",
    "# t = t.clip(nm_airspaces[\"LSAZM7\"].shape.bounds).eval(desc=\"clipping\", max_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipyleaflet import Map, basemaps, MeasureControl, Polyline\n",
    "from ipywidgets import Layout\n",
    "from traffic.data import nm_airspaces\n",
    "\n",
    "map_ = Map(\n",
    "    center=(47.48891, 8.85743),\n",
    "    zoom=8,\n",
    "    basemap=basemaps.CartoDB.Positron,\n",
    "    layout=Layout(width=\"1500px\", height=\"1500px\"),\n",
    ")\n",
    "\n",
    "# get list of length x with unique colors\n",
    "color_cycle = fn.generate_color_list(n_clusters)\n",
    "\n",
    "for flight in t_gen1:\n",
    "    map_.add_layer(flight, weight=1, color = 'red', opacity=0.3)\n",
    "\n",
    "for flight in t:\n",
    "    map_.add_layer(flight, weight=1, color = 'blue', opacity=0.3)\n",
    "\n",
    "map_.add_layer(nm_airspaces[\"LSAZM7\"], weight=2, color='black', fill = False)\n",
    "map_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure(go.Scatter())\n",
    "\n",
    "for flight in t:\n",
    "    fig.add_trace(go.Scatter(\n",
    "        mode=\"lines\",\n",
    "        x=[*range(len(flight.data))],\n",
    "        y=flight.data.altitude,\n",
    "        line=dict(color='rgba(0, 0, 255, 0.3)', width=1)\n",
    "    ))\n",
    "\n",
    "for flight in t_gen1:\n",
    "    fig.add_trace(go.Scatter(\n",
    "        mode=\"lines\",\n",
    "        x=[*range(len(flight.data))],\n",
    "        y=flight.data.altitude,\n",
    "        line=dict(color='rgba(255, 0, 0, 0.3)', width=1)\n",
    "    ))\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# 217 # provided input\n",
    "# 218 f i g = go . Figure (\n",
    "# 219 data=go . S c a t t e r ( x=given . time , y=given . a l t , name=\" input \" , mode=\"markers+ l i n e s \" )\n",
    "# 220 )\n",
    "# 221 # predi c ted futur e a l t i t ud e\n",
    "# 222 f i g . add_t race (\n",
    "# 223 go . S c a t t e r (\n",
    "# 224 x=predi c ted . time , y=predi c ted . a l t , name=\" predi c ted \" , mode=\"markers+ l i n e s \"\n",
    "# 225 )\n",
    "# 226 )\n",
    "# 227 #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reduction to low traffic trajectories-------------------------------------------------\n",
    "def get_lowtraf_trajs(file_load: str,\n",
    "                      path_save: str,\n",
    "                      max_percentile: float = 0.99,\n",
    "                      low_th: float = 0.2,\n",
    "                      max_workers: int = 20,\n",
    "                      resampling: str = '5s') -> None:\n",
    "    \"\"\"\n",
    "    Computes hours of low traffic and reduces the trajectories to these hours. The hours\n",
    "    of low traffic are defined as the hours with a traffic volume below a threshold\n",
    "    which is defined as a fraction of the maximum traffic volume. The maximum traffic\n",
    "    volume is defined as a percentile of flown seconds per hour.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_load : str\n",
    "        Path to the traffic file which will be loaded\n",
    "    path_save : str\n",
    "        Path to the folder where the low traffic trajectories will be saved\n",
    "    max_percentile : float, optional\n",
    "        Percentile which is defined as max traffic, by default 0.99\n",
    "    low_th : float, optional\n",
    "        Threshold expressed as a fraction of max traffic below which an hour will be\n",
    "        labeled as low traffic-hour, by default 0.2\n",
    "    max_workers : int, optional\n",
    "        Max amount of workers for multi-processing of resampling and id assignment, \n",
    "        by default 20\n",
    "    resampling : str, optional\n",
    "        Resampling interval applied to the trajectories, by default '5s'\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    trajs = Traffic.from_file(file_load)\n",
    "    # Id assignment and resampling\n",
    "    trajs = trajs.assign_id().eval()\n",
    "    # Aggregation on trajectory level, computation of stay time and hour entered\n",
    "    df = trajs.resample(resampling).eval(desc='processing', max_workers=max_workers).data\n",
    "    df = df.groupby('flight_id')['timestamp'].agg(['min', 'max']).reset_index()\n",
    "    df = df.rename({'min': 'in', 'max': 'out'}, axis=1)\n",
    "    df['stay_s'] = (df['out'] - df['in']).dt.total_seconds()\n",
    "    df['timestamp_entered_h'] = df['in'].dt.floor('h')\n",
    "    df = df.drop(['in','out'], axis=1)\n",
    "    # Aggreagation on hourly level\n",
    "    hourly_time = df.groupby(['timestamp_entered_h'])['stay_s'].sum()\n",
    "    hourly_count = df.groupby(['timestamp_entered_h'])['flight_id'].count()\n",
    "    hourly_df = pd.concat([hourly_time, hourly_count], axis=1)\n",
    "    hourly_df = hourly_df.rename({'flight_id': 'count'}, axis=1)\n",
    "    # Rescaling and identification of hours below threshold\n",
    "    hourly_df= hourly_df/hourly_df.quantile(max_percentile)\n",
    "    hourly_df['low'] = hourly_df['stay_s'].apply(lambda x: 'yes' if x < low_th and x >= 0 else 'no')\n",
    "    low_hours = hourly_df[hourly_df.low == 'yes'].index\n",
    "    # Reduction of trajectories to low traffic hours\n",
    "    ids_use = df[df.timestamp_entered_h.isin(low_hours)].flight_id.to_numpy()\n",
    "    trajs_use = trajs[ids_use]\n",
    "    # create path_save if it does not exist\n",
    "    if os.path.isdir(path_save) == False:\n",
    "        os.mkdir(path_save)\n",
    "    # save the Traffic object as a parquet file\n",
    "    trajs_use.to_parquet(f'{path_save}/low_traffic.parquet')\n",
    "\n",
    "# Restructuring to training data--------------------------------------------------------------------\n",
    "\n",
    "def get_training_data(file_load: str,\n",
    "                        path_save: str,\n",
    "                        max_workers: int = 20) -> None:\n",
    "    \"\"\"\n",
    "    Restructures the trajectories to training data. The trajectories are resampled to 100 data\n",
    "    points per trajectory. The data points are interpolated linearly and the resulting trajectories\n",
    "    are converted to numpy arrays min-max rescaling is applied. The resulting numpy arrays\n",
    "    (normalized and non-normalized) are saved along with a txt file containing the min and max\n",
    "    values used for the normalization.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_load : str\n",
    "        Path to the traffic file which will be loaded\n",
    "    path_save : str\n",
    "        Path to the folder where the created files will be saved\n",
    "    max_workers : int, optional\n",
    "        Max amount of workers for multi-processing of resampling, by default 20\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load data\n",
    "    trajs = Traffic.from_file(file_load)\n",
    "\n",
    "    # Resample to 100 data points per trajectory\n",
    "    trajs = trajs.resample(100).eval(max_workers=max_workers)\n",
    "\n",
    "    # Convert to numpy array\n",
    "    X = []\n",
    "    for flight in tqdm(trajs):\n",
    "        df = flight.data[['latitude', 'longitude', 'altitude']]\n",
    "        df = df.interpolate(method='linear', limit_direction='both').ffill().bfill()\n",
    "        df_as_np = df.to_numpy()\n",
    "        X.append(df_as_np)\n",
    "    X = np.array(X)\n",
    "\n",
    "    # Remove potential NaNs\n",
    "    indexList_X_nan = [np.any(i) for i in np.isnan(X)]\n",
    "    X = np.delete(X, indexList_X_nan, axis=0)\n",
    "    X.shape\n",
    "\n",
    "    # Min-Max Normalization\n",
    "    lat_max = np.max(X[:,:,0])\n",
    "    lat_min = np.min(X[:,:,0])\n",
    "    lon_max = np.max(X[:,:,1])\n",
    "    lon_min = np.min(X[:,:,1])\n",
    "    alt_max = np.max(X[:,:,2])\n",
    "    alt_min = np.min(X[:,:,2])\n",
    "    X_norm = X.copy() \n",
    "    X_norm[:,:,0] = (X_norm[:,:,0] - lat_min) / (lat_max - lat_min)\n",
    "    X_norm[:,:,1] = (X_norm[:,:,1] - lon_min) / (lon_max - lon_min)\n",
    "    X_norm[:,:,2] = (X_norm[:,:,2] - alt_min) / (alt_max - alt_min)\n",
    "\n",
    "    if not os.path.exists(path_save):\n",
    "        os.makedirs(path_save)\n",
    "    np.save(f'{path_save}/X', X)\n",
    "    np.save(f'{path_save}/X_norm', X_norm)\n",
    "    with open(f\"{path_save}/normalisation.txt\", \"w\") as f:\n",
    "        f.write(\n",
    "            'lat_min'\n",
    "            + \" \"\n",
    "            + 'lat_max'\n",
    "            + \" \"\n",
    "            + 'lon_min'\n",
    "            + \" \"\n",
    "            + 'lon_max'\n",
    "            + \" \"\n",
    "            + 'alt_min'\n",
    "            + \" \"\n",
    "            + 'alt_max'\n",
    "            + f'\\n{lat_min}'\n",
    "            + \" \"\n",
    "            + str(lat_max)\n",
    "            + \" \"\n",
    "            + str(lon_min)\n",
    "            + \" \"\n",
    "            + str(lon_max)\n",
    "            + \" \"\n",
    "            + str(alt_min)\n",
    "            + \" \"\n",
    "            + str(alt_max)\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "# Visualisation-------------------------------------------------------------------------------------\n",
    "def generate_color_list(num_colors):\n",
    "    # Generate a list of evenly spaced hues\n",
    "    hues = [i / float(num_colors) for i in range(num_colors)]\n",
    "\n",
    "    # Shuffle the list of hues\n",
    "    random.shuffle(hues)\n",
    "\n",
    "    # Convert the hues to RGB colors\n",
    "    colors = [tuple(int(i * 255) for i in colorsys.hsv_to_rgb(hue, 0.8, 0.8)) for hue in hues]\n",
    "\n",
    "    # Convert the RGB colors to hex strings\n",
    "    hex_colors = [f\"#{r:02x}{g:02x}{b:02x}\" for r, g, b in colors]\n",
    "\n",
    "    return hex_colors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vt_2_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
